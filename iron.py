# -*- coding: utf-8 -*-
"""iron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zGcG6z41uQ-S7svRxe16lMY_DdhJYpBe
"""

import pandas as pd
import numpy as np
import zipfile

local_zip='/content/Participants_Data_TGIH.zip'
zip_ref=zipfile.ZipFile(local_zip,'r')
zip_ref.extractall()
zip_ref.close()

df=pd.read_csv('/content/Participants_Data_TGIH/Train.csv')
df.head()

df.info()

df=df.drop_duplicates()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from matplotlib import style
style.use('ggplot')
from scipy import stats
pd.options.display.max_columns=50
from matplotlib.pylab import rcParams 
rcParams['figure.figsize']=10,6

df1=np.log1p(df['UnitPrice'])

df.drop("UnitPrice", axis = 1, inplace = True)

!pip install category_encoders

from category_encoders import MEstimateEncoder

df=df.astype({'StockCode': 'category','Description': 'category'})

category_list=['StockCode','Description']

encoder_final=MEstimateEncoder()
encoder_final.fit(df[category_list], df1)

cat_enc = encoder_final.transform(df[category_list], df1)
continuous_train = df.drop(columns= category_list)
df = pd.concat([cat_enc,continuous_train],axis=1)

test_enc=encoder_final.transform(df2[category_list])
continuous_test=df2.drop(columns= category_list)
df2=pd.concat([test_enc,continuous_test],axis=1)

df2.head()

sns.distplot(np.log1p(df['Quantity']))

from scipy import stats 
y=np.log1p(df['UnitPrice'])
pd.Series(y).skew()

q1,q2=np.percentile(df['UnitPrice'],[25,75])
iqr=q2-q1
l=q1-(1.5*iqr)
u=q2+(1.5*iqr)
print(l,u)
plt.hist(df['UnitPrice'])
plt.show()

sns.heatmap(df.corr(),annot=True)

df['Quantity'].unique()

cols=['Quantity']

for col in cols:
    df[col][df[col] < 0] = -(df[col][df[col] < 0])

#df['Year'] = pd.to_datetime(df['InvoiceDate']).dt.year

#df['Month'] = pd.to_datetime(df['InvoiceDate']).dt.month
#df['Day'] = pd.to_datetime(df['InvoiceDate']).dt.day

#df['Dayofweek'] = pd.to_datetime(df['InvoiceDate']).dt.dayofweek

#df['DayOfyear'] = pd.to_datetime(df['InvoiceDate']).dt.dayofyear

df['Week'] = pd.to_datetime(df['InvoiceDate']).dt.week

#df['Quarter'] = pd.to_datetime(df['InvoiceDate']).dt.quarter 
#df['Is_month_start'] = pd.to_datetime(df['InvoiceDate']).dt.is_month_start

#df['Is_month_end'] = pd.to_datetime(df['InvoiceDate']).dt.is_month_end

#df['Is_quarter_start'] = pd.to_datetime(df['InvoiceDate']).dt.is_quarter_start

#df['Is_quarter_end'] = pd.to_datetime(df['InvoiceDate']).dt.is_quarter_end
#df['Is_year_start'] = pd.to_datetime(df['InvoiceDate']).dt.is_year_start

#df['Is_year_end'] = pd.to_datetime(df['InvoiceDate']).dt.is_year_end

#df['Semester'] = np.where(df['Quarter'].isin([1,2]),1,2)

#df['Is_weekend'] = np.where(df['Dayofweek'].isin([5,6]),1,0)

#df['Is_weekday'] = np.where(df['Dayofweek'].isin([0,1,2,3,4]),1,0)

#df['Days_in_month'] = pd.to_datetime(df['InvoiceDate']).dt.days_in_month
    
df['Hour'] = pd.to_datetime(df['InvoiceDate']).dt.hour
df['min'] = pd.to_datetime(df['InvoiceDate']).dt.minute
df['sec'] = pd.to_datetime(df['InvoiceDate']).dt.second


#df['Time'] = [((date.hour*60+(date.minute))*60)+date.second for date in df.InvoiceDate]
df['Time']=(df['Hour']*3600)+(df['min']*60)+df['sec']

from statsmodels.stats.outliers_influence import variance_inflation_factor

def calc_vif(X):

    # Calculating VIF
    vif = pd.DataFrame()
    vif["variables"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

    return(vif)

calc_vif(df)

X = df.iloc[:,:-1]
calc_vif(X)

df.head()

#df['Unique_sku_per_store']=df.groupby(['InvoiceNo'])['StockCode'].transform('nunique')

#df['Unique_sku_per_store1']=df.groupby(['InvoiceNo'])['Description'].transform('nunique')

df['Unique_sku_per_store2']=df.groupby(['InvoiceNo'])['Quantity'].transform('nunique')
#df['Unique_sku_per_store3']=df.groupby(['InvoiceNo'])['Quantity'].transform('mean')
#df['Unique_sku_per_store4']=df.groupby(['InvoiceNo'])['Quantity'].transform('median')
#df['Unique_sku_per_store5']=df.groupby(['InvoiceNo'])['Quantity'].transform('max')
#df['Unique_sku_per_store6']=df.groupby(['InvoiceNo'])['Quantity'].transform('min')
#df['Unique_sku_per_store7']=df.groupby(['InvoiceNo'])['Quantity'].transform('std')

#df['Unique_sku_per_store3']=df.groupby(['CustomerID'])['StockCode'].transform('count')

#df['Unique_sku_per_store4']=df.groupby(['CustomerID'])['Description'].transform('count')

df['Unique_sku_per_store8']=df.groupby(['CustomerID'])['Quantity'].transform('nunique')
#df['Unique_sku_per_store9']=df.groupby(['CustomerID'])['Quantity'].transform('mean')

#df['Unique_sku_per_store10']=df.groupby(['CustomerID'])['Quantity'].transform('median')

#df['Unique_sku_per_store11']=df.groupby(['CustomerID'])['Quantity'].transform('max')
#df['Unique_sku_per_store12']=df.groupby(['CustomerID'])['Quantity'].transform('min')
#df['Unique_sku_per_store13']=df.groupby(['CustomerID'])['Quantity'].transform('std')

#df['Unique_sku_per_store6']=df.groupby(['CustomerID'])['Quantity'].transform('count')

df['Unique_sku_per_store3']=df.groupby(['StockCode','Week'])['Quantity'].transform('nunique')
df['Unique_sku_per_store4']=df.groupby(['Description','Week'])['Quantity'].transform('nunique')

df['Unique_sku_per_store']=df.groupby(['StockCode','Description'])['Quantity'].transform('nunique')
#df['Unique_sku_per_store1']=df.groupby(['Country'])['InvoiceNo'].transform('nunique')

df['Unique_sku_per_store3']=df.groupby(['StockCode','Description'])['Quantity'].transform('mean')
df['Unique_sku_per_store4']=df.groupby(['StockCode','Description'])['Quantity'].transform('median')
df['Unique_sku_per_store5']=df.groupby(['StockCode','Description'])['Quantity'].transform('min')
df['Unique_sku_per_store6']=df.groupby(['StockCode','Description'])['Quantity'].transform('max')
df['Unique_sku_per_store7']=df.groupby(['StockCode','Description'])['Quantity'].transform('std')

df['CustomerID']=df['CustomerID'].astype(int)

df2=pd.read_csv('/content/Participants_Data_TGIH/Test.csv')
df2.head()

df2.describe()

df3=pd.read_csv('/content/Participants_Data_TGIH/Sample Submission.csv')
df3.head()

df.drop("InvoiceDate", axis = 1, inplace = True)

df.drop("CustomerID", axis = 1, inplace = True)
df.drop("sec", axis = 1, inplace = True)
df.drop("Country", axis = 1, inplace = True)
df.drop("InvoiceNo", axis = 1, inplace = True)

df.drop("Hour", axis = 1, inplace = True)

df.drop("InvoiceDate", axis = 1, inplace = True)

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split

from sklearn.metrics import mean_squared_error

train_x,test_x,train_y,test_y=train_test_split(df,df1,test_size=0.25,random_state=0)

model=RandomForestRegressor()
model.fit(train_x,train_y)
o=model.predict(test_x)
print(mean_squared_error(test_y,o))

from lightgbm import LGBMRegressor
from xgboost import XGBRegressor

model_lgb_tuned=LGBMRegressor(bagging_fraction=0.8, bagging_frequency=4, boosting_type='gbdt',
              class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
              importance_type='split', learning_rate=0.1, max_depth=30,
              min_child_samples=20, min_child_weight=30, min_data_in_leaf=70,
              min_split_gain=0.0001, n_estimators=200, n_jobs=-1,
              num_leaves=1200, objective=None, random_state=None, reg_alpha=0.0,
              reg_lambda=0.0, silent=True, subsample=1.0,
              subsample_for_bin=200000, subsample_freq=0)

model_lgb_tuned.fit(train_x,train_y)
o2=model_lgb_tuned.predict(test_x)
print(np.sqrt(mean_squared_error(test_y,o2)))

feat_importances = pd.Series(model.feature_importances_)
feat_importances.nlargest(20).plot(kind='barh')
plt.show()

print(np.sqrt(mean_squared_error(test_y,o)))

!pip install catboost

import catboost as cat

model2=cat.CatBoostRegressor()
model2.fit(train_x,train_y)
o3=model2.predict(test_x)
print(np.sqrt(mean_squared_error(test_y,o3)))

for col in cols:
    df2[col][df2[col] < 0] = -(df2[col][df2[col] < 0])

#df2['Year'] = pd.to_datetime(df2['InvoiceDate']).dt.year

#df2['Month'] = pd.to_datetime(df2['InvoiceDate']).dt.month
#df2['Day'] = pd.to_datetime(df2['InvoiceDate']).dt.day

#df2['Dayofweek'] = pd.to_datetime(df2['InvoiceDate']).dt.dayofweek

#df2['DayOfyear'] = pd.to_datetime(df2['InvoiceDate']).dt.dayofyear

df2['Week'] = pd.to_datetime(df2['InvoiceDate']).dt.week

#df2['Quarter'] = pd.to_datetime(df2['InvoiceDate']).dt.quarter 
#df2['Is_month_start'] = pd.to_datetime(df2['InvoiceDate']).dt.is_month_start

#df2['Is_month_end'] = pd.to_datetime(df2['InvoiceDate']).dt.is_month_end

#df2['Is_quarter_start'] = pd.to_datetime(df2['InvoiceDate']).dt.is_quarter_start

#df2['Is_quarter_end'] = pd.to_datetime(df2['InvoiceDate']).dt.is_quarter_end
#df2['Is_year_start'] = pd.to_datetime(df2['InvoiceDate']).dt.is_year_start

#df2['Is_year_end'] = pd.to_datetime(df2['InvoiceDate']).dt.is_year_end

#df2['Semester'] = np.where(df2['Quarter'].isin([1,2]),1,2)

#df2['Is_weekend'] = np.where(df2['Dayofweek'].isin([5,6]),1,0)

#df2['Is_weekday'] = np.where(df2['Dayofweek'].isin([0,1,2,3,4]),1,0)

#df2['Days_in_month'] = pd.to_datetime(df2['InvoiceDate']).dt.days_in_month
    
df2['Hour'] = pd.to_datetime(df2['InvoiceDate']).dt.hour
df2['min'] = pd.to_datetime(df2['InvoiceDate']).dt.minute
df2['sec'] = pd.to_datetime(df2['InvoiceDate']).dt.second
df2['Time']=(df2['Hour']*3600)+(df2['min']*60)+df2['sec']

df2.drop("InvoiceDate", axis = 1, inplace = True)

df2['CustomerID']=df2['CustomerID'].astype(int)

#df2['Unique_sku_per_store']=df2.groupby(['InvoiceNo'])['StockCode'].transform('nunique')

#df2['Unique_sku_per_store1']=df2.groupby(['InvoiceNo'])['Description'].transform('nunique')

df2['Unique_sku_per_store2']=df2.groupby(['InvoiceNo'])['Quantity'].transform('nunique')
#df2['Unique_sku_per_store3']=df2.groupby(['CustomerID'])['StockCode'].transform('count')

#df2['Unique_sku_per_store4']=df2.groupby(['CustomerID'])['Description'].transform('count')

df2['Unique_sku_per_store8']=df2.groupby(['CustomerID'])['Quantity'].transform('nunique')
#df2['Unique_sku_per_store6']=df2.groupby(['CustomerID'])['Quantity'].transform('count')

#df2['Unique_sku_per_store7']=df2.groupby(['Country'])['Description'].transform('nunique')
#df2['Unique_sku_per_store8']=df2.groupby(['Country'])['Quantity'].transform('nunique')
df2['Unique_sku_per_store']=df2.groupby(['StockCode','Description'])['Quantity'].transform('nunique')

df2['Unique_sku_per_store7']=df2.groupby(['StockCode'])['Quantity'].transform('nunique')
df2['Unique_sku_per_store8']=df2.groupby(['Description'])['Quantity'].transform('nunique')

df2['Unique_sku_per_store3']=df2.groupby(['StockCode','Description'])['Quantity'].transform('mean')
df2['Unique_sku_per_store4']=df2.groupby(['StockCode','Description'])['Quantity'].transform('median')
df2['Unique_sku_per_store5']=df2.groupby(['StockCode','Description'])['Quantity'].transform('min')
df2['Unique_sku_per_store6']=df2.groupby(['StockCode','Description'])['Quantity'].transform('max')

df2.drop("Country", axis = 1, inplace = True)
df2.drop("InvoiceNo", axis = 1, inplace = True)

df2.drop("Hour", axis = 1, inplace = True)

df2['StockCode']= df2.StockCode.map(Encoding )

df2.drop("CustomerID", axis = 1, inplace = True)
df2.drop("sec", axis = 1, inplace = True)

df2.head()

d=model.predict(df2)
h=np.expm1(d)

d1=model2.predict(df2)
h1=np.expm1(d1)

import math 
for i in range(len(h)):
  h[i]=(round(h[i],2))

df5=pd.DataFrame(h)

df5.head()


# -*- coding: utf-8 -*-
"""crossbell.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mDvJU3Bqljf4nRHQIk3WCAbwmZomRGgo
"""

import zipfile
local_zip='/content/train.csv_VsW9EGx.zip'
zip_ref=zipfile.ZipFile(local_zip,'r')
zip_ref.extractall()
zip_ref.close()

import pandas as pd
import numpy as np

df=pd.read_csv('/content/train.csv')
df.describe()

df.drop('id',axis=1,inplace=True)
df2.drop('id',axis=1,inplace=True)

train=df
test=df2
train.drop_duplicates(keep='first',inplace = True)
train.duplicated().sum()

cat_cols = train.select_dtypes(include = 'object')
num_cols = train.select_dtypes(include=['int64','float64'])

cat_cols

combine = train.append(test)

combine['Vintage'] = combine['Vintage']/365



combine['Vehicle_Age']=combine['Vehicle_Age'].replace({'< 1 Year':0,'1-2 Year':1,'> 2 Years':2})



combine['Vehicle_Damage']=combine['Vehicle_Damage'].replace({'Yes':1,'No':0})
combine['Gender']=combine['Gender'].replace({'Male':1,'Female':0})



#Analysis show that people in Age Group 30-60 have higher response rate so creating a separate feature
combine['Age_Group'] = np.where((combine['Age']<30) & (combine['Age'] > 60),0,1)

combine.head()

def find_non_rare_labels(df, variable, tolerance):
    
    temp = df.groupby([variable])[variable].count() / len(df)
    
    non_rare = [x for x in temp.loc[temp>tolerance].index.values]
    
    return non_rare

def rare_encoding(data, variable, tolerance,new_col):
    frequent_cat = find_non_rare_labels(data, variable, tolerance)

    # re-group rare labels
    data[new_col] = np.where(data[variable].isin(
        frequent_cat), data[variable], 'Rare')



for variable in ['Policy_Sales_Channel']:
    
     rare_encoding(combine, variable, 0.01,'Policy_Sales_Channel_Group')
        
for variable in ['Region_Code']:
    
     rare_encoding(combine, variable, 0.02,'Region_Code_Group')

combine['IsPreviouslyInsuredandVehicleDamaged'] = np.where((combine['Previously_Insured']==0) & (combine['Vehicle_Damage']==1),1,0)
combine['IsVehicleDamagedandDrivingLicense'] = np.where((combine['Vehicle_Damage']==1) & (combine['Driving_License']==1),1,0)
combine['TotalAmountPaidTillDate'] = combine['Annual_Premium']*combine['Vintage']
combine['PremiumperRegion'] = combine.groupby('Region_Code')['Annual_Premium'].transform('mean')
combine['PremiumperPolicy_Sales_Channel'] = combine.groupby('Policy_Sales_Channel')['Annual_Premium'].transform('mean')
combine['AvgVehicleAgePerRegion'] = combine.groupby('Policy_Sales_Channel')['Annual_Premium'].transform('mean')
combine['AvgCustomerAgeRegionWise'] = combine.groupby('Region_Code')['Age'].transform('mean')
combine['AvgCustomerAgeSaleChannelWise'] = combine.groupby('Policy_Sales_Channel')['Age'].transform('mean')
combine['SaleChannelsPerRegion'] = combine.groupby('Region_Code')['Policy_Sales_Channel'].transform('nunique')
combine['RegionwisePreviouslyInsured'] = combine.groupby('Region_Code')['Previously_Insured'].transform('count')
combine['RegionwiseVintage'] = combine.groupby('Region_Code')['Vintage'].transform('mean').astype('int')
combine['SaleChannelwiseVintage'] = combine.groupby('Policy_Sales_Channel')['Vintage'].transform('mean').astype('int')

combine['AvgRegionGenderWisePremium'] = combine.groupby(['Region_Code','Gender'])['Annual_Premium'].transform('mean')
combine['NoPeoplePrevInsuredRegionGenderWise'] = combine.groupby(['Region_Code','Gender'])['Previously_Insured'].transform('count')
combine['NoPeoplePrevInsuredSalesChannelGenderWise'] = combine.groupby(['Policy_Sales_Channel','Gender'])['Previously_Insured'].transform('count')
combine['NoPeoplePrevInsuredSalesChannelRegionWise'] = combine.groupby(['Region_Code','Policy_Sales_Channel'])['Previously_Insured'].transform('count')
combine['AvgCustomerDurationRegionGenderWise'] = combine.groupby(['Region_Code','Gender'])['Vintage'].transform('mean')

combine['InsuranceLicense'] = combine['Driving_License'].astype('str') + '' + combine['Previously_Insured'].astype('str')
combine['InsuranceGender'] = combine['Gender'].astype('str') + '' + combine['Previously_Insured'].astype('str')

combine['Region_Code']=combine['Region_Code'].astype(int)
combine['Policy_Sales_Channel']=combine['Policy_Sales_Channel'].astype(int)

cat_col=['Gender','Driving_License', 'Region_Code', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage','Policy_Sales_Channel','InsuranceLicense','InsuranceGender','Policy_Sales_Channel_Group','Region_Code_Group']

combine.head(20)

train = combine[combine['Response'].isnull()!= True]
test = combine[combine['Response'].isnull()== True]

test.drop(['Response'],axis=1,inplace=True)

X = train.drop(["Response"], axis=1)
Y = train["Response"]

from sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder, OneHotEncoder
from sklearn.model_selection import cross_val_score, cross_val_predict,KFold, StratifiedKFold
from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score

oof_pred               = np.zeros((len(train),))
y_pred_final           = np.zeros((len(test),))
num_models             = 3

n_splits               = 20
error                  = []

kf=StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=294)
    
for i,(train_idx,val_idx) in enumerate(kf.split(X,Y)):    
    
    wghts                     = [0]*num_models
    test_roc_score            = []
    
    
    X_train, y_train = X.iloc[train_idx,:], Y.iloc[train_idx]

    X_val, y_val = X.iloc[val_idx, :], Y.iloc[val_idx]
    

    print('\nFold: {}\n'.format(i+1))

    model1 = CatBoostClassifier(learning_rate = 0.03,random_state=42,scale_pos_weight=7, custom_metric=['AUC'])
    model1.fit(X_train,y_train,cat_features=cat_col,eval_set=(X_val, y_val),early_stopping_rounds=30,verbose=100)
    testpred1 = model1.predict_proba(X_val)[:,1]
    test_roc_score.append(roc_auc_score(y_val, testpred1))
    print("Test ROC AUC for model 1: %.4f"%(roc_auc_score(y_val, testpred1)))
    
    model2 = CatBoostClassifier(learning_rate = 0.04,random_state=42,scale_pos_weight=7, custom_metric=['AUC'])
    model2.fit(X_train,y_train,cat_features=cat_col,eval_set=(X_val, y_val),early_stopping_rounds=40,verbose=100)
    testpred2 = model2.predict_proba(X_val)[:,1]
    test_roc_score.append(roc_auc_score(y_val, testpred2))
    print("Test ROC AUC for model 2: %.4f"%(roc_auc_score(y_val, testpred2)))
    
    model3 = CatBoostClassifier(learning_rate = 0.05,random_state=42,scale_pos_weight=7, custom_metric=['AUC'])
    model3.fit(X_train,y_train,cat_features=cat_col,eval_set=(X_val, y_val),early_stopping_rounds=20,verbose=100)
    testpred3 = model3.predict_proba(X_val)[:,1]
    test_roc_score.append(roc_auc_score(y_val, testpred3))
    print("Test ROC AUC for model 3: %.4f"%(roc_auc_score(y_val, testpred3)))
    
    wghts              = np.exp(-1000*np.array(test_roc_score/sum(test_roc_score)))
    wghts              = wghts/sum(wghts)
    
    val_pred           = wghts[0]*testpred1+wghts[1]*testpred2 +wghts[2]*testpred3
    print('validation roc_auc_score fold-',i+1,': ',roc_auc_score(y_val, val_pred))
    
    oof_pred[val_idx]  = val_pred
    y_pred_final += (wghts[0]*model1.predict_proba(test)[:,1]+wghts[1]*model2.predict_proba(test)[:,1]+wghts[2]*model3.predict_proba(test)[:,1])/(n_splits)
    
    print('\n')
    
print('OOF ROC_AUC_Score:- ',(roc_auc_score(Y,oof_pred)))

temp, age_temp = list(df['Age'].values), list()
temp1 = list(df['Gender'].values)

for age in range(len(temp)):
  i = temp[age]
  k = temp1[age]
  if i <=33 and k==0:
      age_temp.append(0)
  elif i <=46 and k==0:
      age_temp.append(1)
  elif i <=59 and k==0:
      age_temp.append(2)
  elif i <=72 and k==0:
      age_temp.append(3)
  elif i <=85 and k==0:
      age_temp.append(4)
  elif i <=33 and k==1:
      age_temp.append(5)
  elif i <=46 and k==1:
      age_temp.append(6)
  elif i <=59 and k==1:
      age_temp.append(7)
  elif i <=72 and k==1:
      age_temp.append(8)
  elif i <=85 and k==1:
      age_temp.append(9)
        
    
    

df['Age_Prof'] = age_temp

agg_func = {
    'Annual_Premium': ['mean','std','sum','median']    
}
agg_func = df.groupby('Previously_Insured').agg(agg_func)
agg_func.columns = [ 'Previously_Insured_' + ('_'.join(col).strip()) for col in agg_func.columns.values]
agg_func.reset_index(inplace=True)
df = df.merge(agg_func, on=['Previously_Insured'], how='left')

df.head()

df['hospitals678'].value_counts()

df[''].value_counts().sort_values()

region_map = {'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2}
df['Vehicle_Age'] = df['Vehicle_Age'].map(region_map)

df['Gender']=df['Gender'].replace({'Male':1,'Female':0})
df['Vehicle_Damage']=df['Vehicle_Damage'].replace({'Yes':1,'No':0})

cat_cols=['Gender','Driving_License', 'Region_Code', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage','Policy_Sales_Channel','Age_Prof1','Age_Prof']

df['Region_Code']=df['Region_Code'].astype(int)

df['Policy_Sales_Channel']=df['Policy_Sales_Channel'].astype(int)

temp, age_temp = list(df['Policy_Sales_Channel'].values), list()
temp1 = list(df['Vehicle_Age'].values)

for age in range(len(temp)):
  i = temp[age]
  k = temp1[age]
  if i <=54 and k==0:
      age_temp.append(0)
  elif i <=108 and k==0:
      age_temp.append(1)
  elif i <=163 and k==0:
      age_temp.append(2)
  elif i <=54 and k==1:
      age_temp.append(3)
  elif i <=108 and k==1:
      age_temp.append(4)
  elif i <=163 and k==1:
      age_temp.append(5)
  elif i <=54 and k==2:
      age_temp.append(6)
  elif i <=108 and k==2:
      age_temp.append(7)
  elif i <=163 and k==2:
      age_temp.append(8)
        
    
    

df['Age_Prof1'] = age_temp

df['mean_Ad122']=df.groupby(['Vehicle_Age'])['Vintage'].transform('min')

df['mean_A']=df.groupby(['Age'])['Region_Code'].transform('count')
df['mean_Ad']=df.groupby(['Vehicle_Age'])['Policy_Sales_Channel'].transform('count')
df['mean_A1']=df['Vehicle_Age']+df['Vehicle_Damage']
df['mean_Ad1']=df['Previously_Insured']+df['Driving_License']

df['mean_A']=df.groupby(['Region_Code'])['Annual_Premium'].transform('mean')
df['mean_Ad']=df.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('mean')
df['mean_Adm']=df.groupby(['Vintage'])['Annual_Premium'].transform('mean')
df['mean_Admi']=df.groupby(['Vehicle_Age'])['Annual_Premium'].transform('mean')

df['mean_A1']=df.groupby(['Region_Code'])['Annual_Premium'].transform('sum')
df['mean_Ad1']=df.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('sum')
df['mean_Adm1']=df.groupby(['Vintage'])['Annual_Premium'].transform('sum')
df['mean_Admi1']=df.groupby(['Vehicle_Age'])['Annual_Premium'].transform('sum')

df['mean_A2']=df.groupby(['Region_Code'])['Annual_Premium'].transform('max')
df['mean_Ad2']=df.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('max')
df['mean_Adm2']=df.groupby(['Vintage'])['Annual_Premium'].transform('max')
df['mean_Admi2']=df.groupby(['Vehicle_Age'])['Annual_Premium'].transform('max')

df['mean_A3']=df.groupby(['Region_Code'])['Annual_Premium'].transform('min')
df['mean_Ad3']=df.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('min')
df['mean_Adm3']=df.groupby(['Vintage'])['Annual_Premium'].transform('min')
df['mean_Admi3']=df.groupby(['Vehicle_Age'])['Annual_Premium'].transform('min')

df.head()

import zipfile
local_zip='/content/test.csv_yAFwdy2.zip'
zip_ref=zipfile.ZipFile(local_zip,'r')
zip_ref.extractall()
zip_ref.close()

df2=pd.read_csv('/content/test.csv')
df2.describe()

temp, age_temp = list(df2['Age'].values), list()
temp1 = list(df2['Gender'].values)

for age in range(len(temp)):
  i = temp[age]
  k = temp1[age]
  if i <=33 and k==0:
      age_temp.append(0)
  elif i <=46 and k==0:
      age_temp.append(1)
  elif i <=59 and k==0:
      age_temp.append(2)
  elif i <=72 and k==0:
      age_temp.append(3)
  elif i <=85 and k==0:
      age_temp.append(4)
  elif i <=33 and k==1:
      age_temp.append(5)
  elif i <=46 and k==1:
      age_temp.append(6)
  elif i <=59 and k==1:
      age_temp.append(7)
  elif i <=72 and k==1:
      age_temp.append(8)
  elif i <=85 and k==1:
      age_temp.append(9)
        
    
    

df2['Age_Prof'] = age_temp

agg_func = {
    'Annual_Premium': ['mean','std','sum','median']    
}
agg_func = df2.groupby('Previously_Insured').agg(agg_func)
agg_func.columns = [ 'Previously_Insured_' + ('_'.join(col).strip()) for col in agg_func.columns.values]
agg_func.reset_index(inplace=True)
df2 = df2.merge(agg_func, on=['Previously_Insured'], how='left')

df2['Annual_Premium'] = np.log1p(df2['Annual_Premium'])
df2['Annual_Premium']= df2['Annual_Premium'].astype("float64")

region_map = {'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2}
df2['Vehicle_Age'] = df2['Vehicle_Age'].map(region_map)


df2['Gender']=df2['Gender'].replace({'Male':1,'Female':0})
df2['Vehicle_Damage']=df2['Vehicle_Damage'].replace({'Yes':1,'No':0})

df2['Region_Code']=df2['Region_Code'].astype(int)

df2['Policy_Sales_Channel']=df2['Policy_Sales_Channel'].astype(int)

temp, age_temp = list(df2['Policy_Sales_Channel'].values), list()
temp1 = list(df2['Vehicle_Age'].values)

for age in range(len(temp)):
  i = temp[age]
  k = temp1[age]
  if i <=54 and k==0:
      age_temp.append(0)
  elif i <=108 and k==0:
      age_temp.append(1)
  elif i <=163 and k==0:
      age_temp.append(2)
  elif i <=54 and k==1:
      age_temp.append(3)
  elif i <=108 and k==1:
      age_temp.append(4)
  elif i <=163 and k==1:
      age_temp.append(5)
  elif i <=54 and k==2:
      age_temp.append(6)
  elif i <=108 and k==2:
      age_temp.append(7)
  elif i <=163 and k==2:
      age_temp.append(8)
        
    
    

df2['Age_Prof1'] = age_temp

df2['mean_A']=df2.groupby(['Region_Code'])['Annual_Premium'].transform('mean')
df2['mean_Ad']=df2.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('mean')
df2['mean_Adm']=df2.groupby(['Vintage'])['Annual_Premium'].transform('mean')
df2['mean_Admi']=df2.groupby(['Vehicle_Age'])['Annual_Premium'].transform('mean')

df2['mean_A1']=df2.groupby(['Region_Code'])['Annual_Premium'].transform('sum')
df2['mean_Ad1']=df2.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('sum')
df2['mean_Adm1']=df2.groupby(['Vintage'])['Annual_Premium'].transform('sum')
df2['mean_Admi1']=df2.groupby(['Vehicle_Age'])['Annual_Premium'].transform('sum')

df2['mean_A2']=df2.groupby(['Region_Code'])['Annual_Premium'].transform('max')
df2['mean_Ad2']=df2.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('max')
df2['mean_Adm2']=df2.groupby(['Vintage'])['Annual_Premium'].transform('max')
df2['mean_Admi2']=df2.groupby(['Vehicle_Age'])['Annual_Premium'].transform('max')

df2['mean_A3']=df2.groupby(['Region_Code'])['Annual_Premium'].transform('min')
df2['mean_Ad3']=df2.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('min')
df2['mean_Adm3']=df2.groupby(['Vintage'])['Annual_Premium'].transform('min')
df2['mean_Admi3']=df2.groupby(['Vehicle_Age'])['Annual_Premium'].transform('min')

df2.drop('id',axis=1,inplace=True)


df2.head()

import seaborn as sns
sns.scatterplot(x=df2['Annual_Premium'],y=df2['Gender'])

sns.heatmap(df.corr(),annot=True)

df['Vintage'].unique()

from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

df.columns

le = LabelEncoder()
cat_cols = ['Gender','Vehicle_Age', 'Vehicle_Damage']
for col in cat_cols:
    df[col] = le.fit_transform(df[col])

df.head()

df.drop('id',axis=1,inplace=True)

df.info()

df=df.astype(float)

IQR = df['Annual_Premium'].quantile(0.75)-df['Annual_Premium'].quantile(0.25)
Upper_Range = df['Annual_Premium'].quantile(0.75)+1.5*IQR
Lower_Range = df['Annual_Premium'].quantile(0.25)-1.5*IQR
print (Upper_Range,Lower_Range)

sns.distplot(df['Annual_Premium'])

df['Annual_Premium'] = np.log1p(df['Annual_Premium'])
df['Annual_Premium']= df['Annual_Premium'].astype("float64")

df.head()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold

import lightgbm as lgb

df1=df['Response']
df.drop('Response',axis=1,inplace=True)

err = [] 
y_pred_tot_lgm = np.zeros((len(df2), 2))


fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=150303)
i = 1

for train_index, test_index in fold.split(df, df1):
    x_train, x_val = df.iloc[train_index], df.iloc[test_index]
    y_train, y_val = df1.iloc[train_index], df1.iloc[test_index]
    m = CatBoostClassifier(n_estimators=10000,
                       random_state=2020,
                       eval_metric='Accuracy',
                       learning_rate=0.1,
                       depth=8,
                       bagging_temperature=0.3,
                       #task_type='GPU'
                       #num_leaves=64
                       
                       )
    m.fit(x_train, y_train,eval_set=[(x_val, y_val)], early_stopping_rounds=100,verbose=200)
    pred_y = m.predict(x_val)
    print(i, " err_lgm: ", roc_auc_score(y_val,pred_y))
    err.append(roc_auc_score(y_val,pred_y))
    y_pred_tot_lgm+= m.predict_proba(df2)[:,1]
    i = i + 1
y_pred_tot_lgm=y_pred_tot_lgm/10
sum(err)/10

from sklearn.metrics import roc_auc_score

train_x,test_x,train_y,test_y=train_test_split(df,df1,test_size=0.2,random_state=151303,stratify=df1,shuffle=True)

model=lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=80, min_data_in_leaf=307, max_iter=1000, max_depth=7, learning_rate=0.1)
    
model.fit(train_x,train_y)
o=model.predict_proba(test_x)[:,1]
print(roc_auc_score(test_y,o))

import xgboost as xgb
from sklearn.svm import SVC
from sklearn.ensemble import ExtraTreesClassifier

model2=ExtraTreesClassifier()
model2.fit(train_x,train_y)
o=model2.predict_proba(test_x)[:,1]
print(roc_auc_score(test_y,o))

from sklearn.neural_network import MLPClassifier
model3=MLPClassifier()
model3.fit(train_x,train_y)
o=model3.predict_proba(test_x)[:,1]
print(roc_auc_score(test_y,o))

model1=HistGradientBoostingClassifier()
model1.fit(train_x,train_y)
o=model1.predict_proba(test_x)[:,1]
print(roc_auc_score(test_y,o))

!sudo pip install hyperopt

!pip install hpsklearn

from hpsklearn import HyperoptEstimator
from hpsklearn import any_classifier
from hpsklearn import any_preprocessing
from hyperopt import tpe

model4=HyperoptEstimator(classifier=any_classifier('cla'),preprocessing=any_preprocessing('pre'),algo=tpe.suggest,max_evals=50,trial_timeout=30)
model4.fit(train_x,train_y)
o=model4.predict_proba(test_x)[:,1]
print(roc_auc_score(test_y,o))

!pip install catboost

from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score
catb = CatBoostClassifier()
catb= catb.fit(train_x,train_y,cat_features=cat_cols,eval_set=(test_x,test_y),plot=True,early_stopping_rounds=30,verbose=200)
y_cat = catb.predict(test_x)
probs_cat_train = catb.predict_proba(test_x)[:, 1]
roc_auc_score(test_y, probs_cat_train)

from sklearn.ensemble import VotingClassifier
from sklearn.experimental import enable_hist_gradient_boosting


from sklearn.ensemble import HistGradientBoostingClassifier

model2=VotingClassifier([('model1',model1),('catb',catb)],voting='soft')

model2.fit(train_x,train_y)
o=model2.predict_proba(test_x)[:,1]
print(roc_auc_score(test_y,o))

for col in cat_cols:
    df2[col] = le.fit_transform(df2[col])

df2.drop('id',axis=1,inplace=True)
df2=df2.astype(float)

pred1=catb.predict_proba(df2)[:, 1]

pred2=(pred+pred1)/2

df3=pd.DataFrame(pred1)

df3.head()

from IPython.display import HTML
import base64  
 

def create_download_link( df, title = "Download CSV file", filename = "ata331.csv"):  
    csv = df.to_csv(index =False)
    b64 = base64.b64encode(csv.encode())
    payload = b64.decode()
    html = '<a download="{filename}" href="data:text/csv;base64,{payload}" target="_blank">{title}</a>'
    html = html.format(payload=payload,title=title,filename=filename)
    return HTML(html)

create_download_link(df3)

import tensorflow as tf
from tensorflow import keras

import os
import tempfile

import matplotlib as mpl
import matplotlib.pyplot as plt


from sklearn.metrics import confusion_matrix

train_df, test_df = train_test_split(df, test_size=0.25)
train_df, val_df = train_test_split(train_df, test_size=0.25)

# Form np arrays of labels and features.
train_labels = np.array(train_df.pop('Response'))
val_labels = np.array(val_df.pop('Response'))
test_labels = np.array(test_df.pop('Response'))

train_features = np.array(train_df)
val_features = np.array(val_df)
test_features = np.array(test_df)

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
]

def make_model(metrics = METRICS, output_bias=None):
  if output_bias is not None:
    output_bias = tf.keras.initializers.Constant(output_bias)
  model = keras.Sequential([
      keras.layers.Dense(
          16, activation='relu',
          input_shape=(train_features.shape[-1],)),
      keras.layers.Dropout(0.5),
      keras.layers.Dense(1, activation='sigmoid',
                         bias_initializer=output_bias),
  ])

  model.compile(
      optimizer=keras.optimizers.Adam(lr=1e-3),
      loss=keras.losses.BinaryCrossentropy(),
      metrics=metrics)

  return model

EPOCHS = 100
BATCH_SIZE = 64

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_auc', 
    verbose=1,
    patience=10,
    mode='max',
    restore_best_weights=True)

model = make_model()
model.summary()

results = model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0)
print("Loss: {:0.4f}".format(results[0]))

initial_bias = np.log([46710/334399])
initial_bias

model = make_model(output_bias = initial_bias)
results = model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0)
print("Loss: {:0.4f}".format(results[0]))

initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')
model.save_weights(initial_weights)

model = make_model()
model.load_weights(initial_weights)
model.layers[-1].bias.assign([0.0])
zero_bias_history = model.fit(
    train_features,
    train_labels,
    batch_size=BATCH_SIZE,
    epochs=20,
    validation_data=(val_features, val_labels), 
    verbose=0)

model = make_model()
model.load_weights(initial_weights)
careful_bias_history = model.fit(
    train_features,
    train_labels,
    batch_size=BATCH_SIZE,
    epochs=20,
    validation_data=(val_features, val_labels), 
    verbose=0)

model = make_model()
model.load_weights(initial_weights)
baseline_history = model.fit(
    train_features,
    train_labels,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    callbacks = [early_stopping],
    validation_data=(val_features, val_labels))

weight_for_0 = (1 /334399)*(381109)/2.0 
weight_for_1 = (1 /46710)*(381109)/2.0

class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))

weighted_model = make_model()
weighted_model.load_weights(initial_weights)

weighted_history = weighted_model.fit(
    train_features,
    train_labels,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    callbacks = [early_stopping],
    validation_data=(val_features, val_labels),
    # The class weights go here
    class_weight=class_weight)

import matplotlib.pyplot as plt


from matplotlib.pylab import rcParams
rcParams['figure.figsize'] = 15,15
import seaborn as sns

x = df[~df.iloc[:,1:].duplicated(keep = 'first')]

#reemove confusing ids

df = df[~df.id.isin(x[x.iloc[:,1:-1].duplicated(keep = False)].id)]

df.shape

def nullColumns(df):
    
    list_of_nullcolumns =[]
    
    for column in df.columns:
        
        total= df[column].isna().sum()
        
        try:
            
            if total !=0:
                
                print('Total Na values is {0} for column {1}' .format(total, column))
                
                list_of_nullcolumns.append(column)
        
        except:
            
            print(column,"-----",total)
    
    print('\n')
    
    return list_of_nullcolumns


def percentMissingFeature(data):
    
    data_na = (data.isnull().sum() / len(data)) * 100
    
    data_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)[:30]
    
    missing_data = pd.DataFrame({'Missing Ratio' :data_na})
    
    return data_na


def plotMissingFeature(data_na):
    
    f, ax = plt.subplots(figsize=(15, 12))
    
    plt.xticks(rotation='90')
    
    if(data_na.empty ==False):
        
        sns.barplot(x=data_na.index, y=data_na)
        
        plt.xlabel('Features', fontsize=15)
        
        plt.ylabel('Percent of missing values', fontsize=15)
        
        plt.title('Percent missing data by feature', fontsize=15)

print('df')

print(nullColumns(df))

print(percentMissingFeature(df))

response = df.loc[:,"Response"].value_counts().rename('Count')
plt.xlabel("Response")
plt.ylabel('Count')
sns.barplot(response.index , response.values).set_title('Response')

sns.distplot(df['Vintage'])

sns.set(style="white", palette="muted", color_codes=True)

f, axes = plt.subplots(2, 1, figsize=(15, 15))

male = df[df['Gender'] =='Male']["Response"].value_counts().rename('Count')

female = df[df['Gender'] =='Female']["Response"].value_counts().rename('Count')

sns.barplot(male.index,male,  color="b", ax=axes[0]).set_title('Gender : Male')

sns.barplot(female.index,female,   color="r", ax=axes[1]).set_title('Gender : Female')

plt.setp(axes, yticks = np.arange(0,50000,10000))

for ax in f.axes:
    
    plt.sca(ax)
    
    plt.xticks(rotation=0)

plt.tight_layout()

sns.set(style="white", palette="muted", color_codes=True)

f, axes = plt.subplots(2, 1, figsize=(15, 15))

dl0 = df[df['Driving_License'] ==0]["Response"].value_counts().rename('Count')

dl1 = df[df['Driving_License'] ==1]["Response"].value_counts().rename('Count')

sns.barplot(dl0.index,dl0,  color="b", ax=axes[0]).set_title('Driving_License : No')

sns.barplot(dl1.index,dl1,   color="r", ax=axes[1]).set_title('Driving_License : Yes')

plt.setp(axes, yticks = np.arange(0,200000,1000))

for ax in f.axes:
    
    plt.sca(ax)
    
    plt.xticks(rotation=0)

plt.tight_layout()

sns.set(style="white", palette="muted", color_codes=True)

f, axes = plt.subplots(2, 1, figsize=(15, 15))

pi0 = df[df['Previously_Insured'] ==0]["Response"].value_counts().rename('Count')

pi1 = df[df['Previously_Insured'] ==1]["Response"].value_counts().rename('Count')

sns.barplot(dl0.index,dl0,  color="b", ax=axes[0]).set_title('Previously_Insured : No')

sns.barplot(dl1.index,dl1,   color="r", ax=axes[1]).set_title('Previously_Insured : Yes')

plt.setp(axes, yticks = np.arange(0,50000,5000))

for ax in f.axes:
    
    plt.sca(ax)
    
    plt.xticks(rotation=0)

plt.tight_layout()

df['Policy_Region'] = df['Policy_Sales_Channel'].astype(str)+'_'+df['Region_Code'].astype(str)


df['Vehicle_Age_License'] = df['Vehicle_Age'].astype(str)+'_'+df['Driving_License'].astype(str)

df.head()


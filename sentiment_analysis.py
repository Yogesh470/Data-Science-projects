# -*- coding: utf-8 -*-
"""sentiment analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fMW3ZLqlySmq_SrKkTui_tntuH7ouKpI
"""

import zipfile
local_zip='/content/Participants_Data_WH20.zip'
zip_ref=zipfile.ZipFile(local_zip,'r')
zip_ref.extractall()
zip_ref.close()

!pip install catboost

import pandas as pd
import numpy as np
import os
import seaborn as sns
import matplotlib.pyplot as plt
import plotly
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import KFold
import warnings
warnings.filterwarnings("ignore")
import re

train = pd.read_csv("/content/Participants_Data_WH20/Train.csv")
test = pd.read_csv("/content/Participants_Data_WH20/Test.csv")
train.head()

train.duplicated().sum()

train['Text_Tag'].value_counts()

train=train.drop_duplicates()
train.info()

train=train.dropna()
train.info()

train["Concat"] = train["Text"].str.cat(train["Text_Tag"].str.strip(), sep=" [SEP] ")
train["Concat"] = train["Concat"].str.strip()
train["Concat"] = train["Concat"].str.lower()

test["Concat"] = test["Text"].str.cat(test["Text_Tag"].str.strip(), sep=" [SEP] ")
test["Concat"] = test["Concat"].str.strip()
test["Concat"] = test["Concat"].str.lower()

train.Concat.values

import re


def clean(x):
  x = re.sub(r'@\w+'," ",x)
  x = re.sub(r'[^a-zA-Z]'," ",x)
  x = re.sub(r' [a-zA-Z]{1} '," ",x)
  return x

merge = pd.concat([train,test]).reset_index(drop=True)
merge["description_word_len"] = merge.apply(lambda x:len(re.findall(r"\w+",x['Concat'])),axis=1)
merge

import re


def clean(x):
  x = re.sub(r'@\w+'," ",x)
  x = re.sub(r'[^a-zA-Z]'," ",x)
  x = re.sub(r' [a-zA-Z]{1} '," ",x)
  return x

merge = pd.concat([train,test]).reset_index(drop=True)
merge["description_word_len"] = merge.apply(lambda x:len(re.findall(r"\w+",x['Product_Description'])),axis=1)
m = merge.groupby('Product_Type')['description_word_len'].agg(['mean','max',"min"])
merge = merge.merge(m,on='Product_Type',how="left")
# merge["Product_Description"] = merge.apply(lambda x:clean(x['Product_Description']),axis=1)

merge

!pip install transformers
!pip install -U sentence-transformers

import tensorflow as tf
import tensorflow.keras.backend as K
from sklearn.model_selection import StratifiedKFold
import tokenizers
print('TF version',tf.__version__)

from sentence_transformers import SentenceTransformer
sentence_embedder = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# sentence_embeddings = sentence_embedder.encode(merge.Concat.values.tolist(),batch_size=64,show_progress_bar=True)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics  import log_loss
from sklearn.model_selection import StratifiedKFold

data = pd.DataFrame(sentence_embeddings)
for i in ["description_word_len",'Labels']:
  data[i] = merge[i].values
data.head()

data = pd.DataFrame(sentence_embeddings)
for i in ["Product_Type","Sentiment","description_word_len","mean","max","min"]:
  data[i] = merge[i].values
data.head()

train = data[~data.Labels.isna()]
test = data[data.Labels.isna()]
test.drop("Labels",axis=1,inplace=True)

X = train.drop(["Labels"],axis=1)
Y = train[['Labels']]

params = {
    "od_type":"Iter",
    "od_wait":180,
    "iterations":25000,
    'learning_rate':0.05,
    "eval_metric":"Accuracy",
    "task_type":"GPU",
    "boosting_type":"Plain"
}

best_score = np.inf
scores = []

folds_large = KFold(n_splits=5,shuffle=True,random_state=1250)

for train_idx , test_idx in folds_large.split(X,Y):
  train_set = (X.iloc[train_idx],Y.iloc[train_idx])
  test_set = (X.iloc[test_idx],Y.iloc[test_idx])

  model = CatBoostClassifier(**params)
  model.fit(*train_set,
            
            eval_set=[test_set],early_stopping_rounds=500,verbose=200)

  score = log_loss(test_set[1].values,model.predict_proba(test_set[0]))
  print(score)
  scores.append(score)
  

  if score < best_score:
    best_score = score
    best_model = model

  print("---"*50)

print(f"Mean Score : {np.array(scores).mean()}")
print(f"Min Score : {np.array(scores).min()}")
print(f"Max Score : {np.array(scores).max()}")

plt.plot(scores)
plt.plot(np.arange(len(scores)),[np.array(scores).mean()]*len(scores),)
plt.show()

model_large = best_model

trainx,trainy,testx,testy=train_test_split(X,Y,test_size=0.1)

testy

model1=XGBClassifier(learning_rate=0.01)

model1.fit(trainx,testx)
score = log_loss(testy,model1.predict_proba(trainy))
score

df = pd.read_csv("/content/Participants_Data_WH20/sample submission.csv")
df.head()

pd.set_option("display.max_colwidth",100)
pd.set_option("display.max_rows",150)
m = pd.DataFrame(list(zip(X.columns,model_large.feature_importances_))).sort_values(1,ascending=False)
m

submission = pd.DataFrame(model_large.predict_proba(test))
submission.columns = [f'Class_{i}' for i in submission.columns]
submission.to_csv("submission_large.csv",index=False)
submission

from sentence_transformers import SentenceTransformer
base_embedder = SentenceTransformer('roberta-base-nli-stsb-mean-tokens')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# base_embeddings = base_embedder.encode(merge.Concat.values.tolist(),batch_size=128,show_progress_bar=True)

data = pd.DataFrame(base_embeddings)
data['Labels'] = merge.Labels.values
data.head()

train = data[~data.Labels.isna()]
test = data[data.Labels.isna()]
test.drop("Labels",axis=1,inplace=True)

X = train.drop(["Labels"],axis=1)
Y = train[['Labels']]

params = {
    "od_type":"Iter",
    "od_wait":150,
    "iterations":25000,
    'learning_rate':0.0355,
    "eval_metric":"Accuracy",
    "task_type":"GPU",
    # "boosting_type":"Plain"
}

best_score = np.inf
scores = []

folds_base = KFold(n_splits=6,shuffle=True,random_state=1350)

for train_idx , test_idx in folds_base.split(X,Y):
  train_set = (X.iloc[train_idx],Y.iloc[train_idx])
  test_set = (X.iloc[test_idx],Y.iloc[test_idx])

  model = CatBoostClassifier(**params)
  model.fit(*train_set,
            
            eval_set=[test_set],early_stopping_rounds=500,verbose=200)

  score = log_loss(test_set[1].values,model.predict_proba(test_set[0]))
  print(score)
  scores.append(score)
  

  if score < best_score:
    best_score = score
    best_model = model

  print("---"*50)

print(f"Mean Score : {np.array(scores).mean()}")
print(f"Min Score : {np.array(scores).min()}")
print(f"Max Score : {np.array(scores).max()}")

plt.plot(scores)
plt.plot(np.arange(len(scores)),[np.array(scores).mean()]*len(scores),)
plt.show()

model_base1 = best_model

submission = pd.DataFrame(model_base1.predict_proba(test))
submission.columns = [f'Class_{i}' for i in submission.columns]
submission.to_csv("submission_base.csv",index=False)
submission

base = pd.read_csv("submission_base.csv")
large = pd.read_csv("submission_large.csv")

final = (base+large)/2
final.to_csv("final.csv",index=False)
final

from IPython.display import HTML
import base64  
 

def create_download_link( df, title = "Download CSV file", filename = "dat51202a.csv"):  
    csv = df.to_csv(index =False)
    b64 = base64.b64encode(csv.encode())
    payload = b64.decode()
    html = '<a download="{filename}" href="data:text/csv;base64,{payload}" target="_blank">{title}</a>'
    html = html.format(payload=payload,title=title,filename=filename)
    return HTML(html)

create_download_link(df3)

import tensorflow as tf
import tensorflow_hub as hub

from sklearn.model_selection import train_test_split

embedding_x = [np.asarray(train[feature_name].values).reshape(-1) for feature_name in TEXT_FEATURES.keys()]



X = embedding_x
y = np.asarray(train["Labels"], dtype=np.uint8).reshape(-1)

LR = 0.0001
BATCH_SIZE = 16
EPOCHS = 100

TEXT_FEATURES = {
    "Concat": None
}

def transformed_name(key):
    return key + '_xf'

def get_model(show_summary=True):
    """
    This function defines a Keras wide and deep model and returns the model as a Keras object.
    """
    
    # adding text input features
    input_texts = []
    for key in TEXT_FEATURES.keys():
        input_texts.append(tf.keras.Input(shape=(1,), name=transformed_name(key), dtype=tf.string))
    
    

    # embed text features
    MODULE_URL = "https://tfhub.dev/google/universal-sentence-encoder/4" # almost a gigabyte of weights, yikes!
    embed = hub.KerasLayer(MODULE_URL)
    reshaped_narrative = tf.reshape(input_texts[0], [-1]) # reshape to match hub layer input, i.e., 1 entry per row
    embed_narrative = embed(reshaped_narrative) # outputs 512 dimensional embeddings
    deep_ff = tf.keras.layers.Reshape((512, ), input_shape=(1, 512))(embed_narrative)
    
    deep = tf.keras.layers.Dense(256, activation='relu')(deep_ff) # deep layers for text features
    deep = tf.keras.layers.Dropout(0.2)(deep)
    deep = tf.keras.layers.Dense(128, activation='relu')(deep)
    deep = tf.keras.layers.Dropout(0.1)(deep)
    deep = tf.keras.layers.Dense(64, activation='relu')(deep)
    deep = tf.keras.layers.Dropout(0.1)(deep)
    #deep = tf.keras.layers.Dense(16, activation='relu')(deep)

    
    output = tf.keras.layers.Dense(6, activation='softmax')(deep)

    inputs = input_texts

    keras_model = tf.keras.models.Model(inputs, output)
    keras_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR),
                        loss='sparse_categorical_crossentropy',  
                        metrics=['accuracy'])
    if show_summary:
        keras_model.summary()

    return keras_model

model = get_model(show_summary=True)

es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)
#lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)
#checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='checkpoints', monitor='val_loss', save_best_only=True, save_weights_only=True)

history = model.fit(x=X, y=y, batch_size=BATCH_SIZE, validation_split=0.1,
                    epochs=EPOCHS, callbacks=[es], verbose=1)

del model
model = get_model(show_summary=False)

model.fit(x=X, y=y, epochs=8, verbose=1, batch_size=BATCH_SIZE) # retrain on the complete dataset f

embedding_x = [np.asarray(test[feature_name].values).reshape(-1) for feature_name in TEXT_FEATURES.keys()]
X = embedding_x

preds = model.predict(X)

df=pd.DataFrame(preds)
df.head()

df2=pd.read_csv('/content/dat5127a.csv')
df2.head()

df4=pd.read_csv('/content/dat5128a.csv')
df4.head()

df5=pd.read_csv('/content/final1.csv')
df5.head()

df.columns = [f'Class_{i}' for i in df.columns]

df3=(df+df5)/2
df3.to_csv("final2.csv",index=False)
df3